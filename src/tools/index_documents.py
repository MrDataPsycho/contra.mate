#!/usr/bin/env python3
"""
Tool to index chunked documents from gold/ layer into OpenSearch vector index.

Pipeline:
1. Read chunked documents from gold layer (JSON format)
2. Convert chunks to VectorModel with embeddings
3. Index into OpenSearch using Vector CRUD service
4. Track processing status in document_indexing_status table

Usage:
    uv run python src/tools/index_documents.py index
    uv run python src/tools/index_documents.py index --limit 10
    uv run python src/tools/index_documents.py index --sample 5
    uv run python src/tools/index_documents.py verify
"""

import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, List

from loguru import logger

import typer
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from sqlmodel import Session, create_engine, select

from contramate.dbs.models.contract import ContractAsmd
from contramate.dbs.models.document_status import (
    DocumentIndexingStatus,
    ProcessingStatus,
)
from contramate.models import ChunkedDocument, EnrichedDocument
from contramate.models.vector import VectorModel, ContentSource
from contramate.services.opensearch_vector_crud_service import OpenSearchVectorCRUDServiceFactory
from contramate.utils.settings.factory import settings_factory

app = typer.Typer(help="Index chunked documents from gold layer into OpenSearch vector database")
console = Console()

# Paths
GOLD_BASE_PATH = Path("data/gold")


def find_json_file(project_id: str, reference_doc_id: str) -> Optional[Path]:
    """
    Find JSON file in gold directory.

    Args:
        project_id: Project identifier
        reference_doc_id: Document identifier

    Returns:
        Path to JSON file or None if not found
    """
    json_file = GOLD_BASE_PATH / project_id / f"{reference_doc_id}.json"
    
    if json_file.exists():
        return json_file
    
    return None


def convert_chunks_to_vector_models(
    document: ChunkedDocument | EnrichedDocument,
    contract_info: ContractAsmd
) -> List[VectorModel]:
    """
    Convert document chunks to VectorModel instances.
    
    Args:
        document: ChunkedDocument or EnrichedDocument
        contract_info: Contract metadata from database
        
    Returns:
        List of VectorModel instances
    """
    vector_models = []
    
    for chunk in document.chunks:
        # Determine content source
        content_source = ContentSource.upload  # Default to upload
        
        # Use enriched content if available (EnrichedDocument)
        content = chunk.enriched_content if hasattr(chunk, 'enriched_content') else chunk.content
        
        vector_model = VectorModel(
            chunk_id=chunk.chunk_index,
            project_id=document.project_id,
            reference_doc_id=document.reference_doc_id,
            document_title=contract_info.document_title or "Unknown Document",
            content_source=content_source,
            contract_type=document.contract_type,
            content=content,
            chunk_index=chunk.chunk_index,
            section_hierarchy=chunk.section_hierarchy,
            char_start=chunk.char_start,
            char_end=chunk.char_end,
            token_count=chunk.token_count,
            has_tables=chunk.has_tables,
            vector=[],  # Will be auto-generated by CRUD service
            created_at=document.created_at
        )
        
        vector_models.append(vector_model)
    
    return vector_models


@app.command()
def index(
    limit: Optional[int] = typer.Option(
        None,
        "--limit",
        "-l",
        help="Limit number of documents to process"
    ),
    sample: Optional[int] = typer.Option(
        None,
        "--sample",
        "-s",
        help="Process only a sample of N documents for testing"
    ),
    skip_existing: bool = typer.Option(
        True,
        "--skip-existing/--reprocess",
        help="Skip documents that are already indexed"
    ),
    delay_seconds: float = typer.Option(
        0.5,
        "--delay",
        "-d",
        help="Delay in seconds between processing each document (for API rate limiting)"
    )
):
    """Index chunked documents from gold layer into OpenSearch vector database"""

    # Create database connection
    postgres_settings = settings_factory.create_postgres_settings()
    connection_string = postgres_settings.connection_string
    engine = create_engine(connection_string, echo=False)

    # Initialize Vector CRUD service
    console.print("[cyan]Initializing OpenSearch Vector CRUD service...[/cyan]")
    try:
        crud_service = OpenSearchVectorCRUDServiceFactory.create_default()
        console.print("[green]✓ Vector CRUD service initialized[/green]\n")
    except Exception as e:
        console.print(f"[red]✗ Failed to initialize Vector CRUD service: {e}[/red]")
        raise typer.Exit(1)

    # Statistics
    total_documents = 0
    processed = 0
    failed = 0
    skipped = 0
    not_found = 0
    total_chunks_indexed = 0
    failed_details = []  # Track failed documents with details

    console.print(f"[cyan]Starting document indexing...[/cyan]")
    console.print(f"[cyan]Source: {GOLD_BASE_PATH}[/cyan]")
    if delay_seconds > 0:
        console.print(f"[cyan]Delay between documents: {delay_seconds}s[/cyan]")
    console.print()

    with Session(engine) as session:
        # Get all documents from contract_asmd
        statement = select(ContractAsmd)
        if limit:
            statement = statement.limit(limit)

        contracts = session.exec(statement).all()
        
        # Sample documents if requested
        if sample and sample < len(contracts):
            import random
            contracts = random.sample(list(contracts), sample)
            console.print(f"[cyan]Selected {sample} random documents for processing[/cyan]")

        total_documents = len(contracts)
        console.print(f"[cyan]Found {total_documents} documents to process[/cyan]\n")

        # Counter for progress reporting
        doc_counter = 0

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console
        ) as progress:
            task = progress.add_task("Indexing documents...", total=total_documents)

            for contract in contracts:
                doc_counter += 1
                project_id = contract.project_id
                reference_doc_id = contract.reference_doc_id

                # Check if already processed
                if skip_existing:
                    existing_status = session.exec(
                        select(DocumentIndexingStatus).where(
                            DocumentIndexingStatus.project_id == project_id,
                            DocumentIndexingStatus.reference_doc_id == reference_doc_id,
                            DocumentIndexingStatus.status == ProcessingStatus.PROCESSED
                        )
                    ).first()

                    if existing_status:
                        skipped += 1
                        progress.update(task, advance=1)
                        continue

                # Find JSON file
                json_file = find_json_file(project_id, reference_doc_id)
                if not json_file:
                    not_found += 1
                    progress.update(task, advance=1)
                    continue

                # Initialize status record
                status_record = DocumentIndexingStatus(
                    project_id=project_id,
                    reference_doc_id=reference_doc_id,
                    status=ProcessingStatus.READY,
                    created_at=datetime.now(timezone.utc),
                    updated_at=datetime.now(timezone.utc)
                )

                # Check if status record already exists
                existing = session.exec(
                    select(DocumentIndexingStatus).where(
                        DocumentIndexingStatus.project_id == project_id,
                        DocumentIndexingStatus.reference_doc_id == reference_doc_id
                    )
                ).first()

                if existing:
                    status_record = existing
                    status_record.status = ProcessingStatus.READY
                    status_record.updated_at = datetime.now(timezone.utc)
                else:
                    session.add(status_record)

                session.commit()

                # Process document
                start_time = time.time()
                try:
                    # Load JSON document
                    try:
                        document = ChunkedDocument.load_json(json_file)
                    except Exception:
                        # Try loading as EnrichedDocument
                        document = EnrichedDocument.load_json(json_file)

                    if not document.chunks:
                        raise ValueError("Document has no chunks")

                    # Convert chunks to VectorModel instances
                    vector_models = convert_chunks_to_vector_models(document, contract)

                    # Index chunks using bulk upsert (prevents duplicates)
                    bulk_result = crud_service.bulk_upsert_documents(
                        vector_models, 
                        auto_embed=True
                    )

                    if bulk_result.is_err():
                        raise RuntimeError(f"Bulk upsert failed: {bulk_result.err()}")

                    bulk_stats = bulk_result.unwrap()
                    
                    if bulk_stats["failed"] > 0:
                        raise RuntimeError(f"Partial upsert failure: {bulk_stats['failed']}/{bulk_stats['total']} chunks failed")

                    # Log upsert statistics
                    inserted = bulk_stats.get("inserted", 0)
                    updated = bulk_stats.get("updated", 0)
                    if inserted > 0 and updated > 0:
                        logger.info(f"Document upsert: {inserted} new chunks inserted, {updated} existing chunks updated")
                    elif inserted > 0:
                        logger.info(f"Document insert: {inserted} new chunks inserted")
                    elif updated > 0:
                        logger.info(f"Document update: {updated} existing chunks updated")

                    execution_time = time.time() - start_time

                    # Update status to PROCESSED
                    app_settings = settings_factory.create_app_settings()
                    status_record.status = ProcessingStatus.PROCESSED
                    status_record.indexed_chunks_count = bulk_stats["success"]
                    status_record.vector_dimension = app_settings.vector_dimension
                    status_record.index_name = crud_service.index_name
                    status_record.execution_time = execution_time
                    status_record.updated_at = datetime.now(timezone.utc)
                    session.add(status_record)
                    session.commit()

                    processed += 1
                    total_chunks_indexed += bulk_stats["success"]

                    # Add delay after successful processing (for API rate limits)
                    if delay_seconds > 0:
                        time.sleep(delay_seconds)

                except Exception as e:
                    execution_time = time.time() - start_time

                    # Update status to FAILED
                    status_record.status = ProcessingStatus.FAILED
                    status_record.execution_time = execution_time
                    status_record.error_message = str(e)[:1000]
                    status_record.updated_at = datetime.now(timezone.utc)
                    session.add(status_record)
                    session.commit()

                    failed += 1
                    # Track failed document details
                    failed_details.append({
                        "project_id": project_id,
                        "reference_doc_id": reference_doc_id,
                        "document_title": contract.document_title or "Unknown",
                        "error": str(e)[:200]
                    })
                    console.print(f"[red]✗ Failed: {contract.document_title[:60] if contract.document_title else 'Unknown'}...[/red]")

                progress.update(task, advance=1)

                # Print progress every 10 documents
                if doc_counter % 10 == 0:
                    progress.stop()
                    console.print(f"\n[bold cyan]Progress Update:[/bold cyan]")
                    console.print(f"  Processed: {doc_counter}/{total_documents} documents")
                    console.print(f"  Success: [green]{processed}[/green] | Failed: [red]{failed}[/red] | Skipped: [yellow]{skipped}[/yellow] | Not Found: [yellow]{not_found}[/yellow]")
                    console.print(f"  Total chunks indexed: [green]{total_chunks_indexed}[/green]\n")
                    progress.start()

    # Print summary
    console.print("\n[bold cyan]Indexing Summary:[/bold cyan]")
    console.print(f"  Total documents: {total_documents}")
    console.print(f"  Successfully indexed: [green]{processed}[/green]")
    console.print(f"  Failed: [red]{failed}[/red]")
    console.print(f"  Skipped (already indexed): [yellow]{skipped}[/yellow]")
    console.print(f"  JSON files not found: [yellow]{not_found}[/yellow]")
    console.print(f"  Total chunks indexed: [green]{total_chunks_indexed}[/green]")

    # Show failed document details
    if failed_details:
        console.print("\n[bold red]Failed Documents:[/bold red]")
        for idx, detail in enumerate(failed_details, 1):
            console.print(f"\n[red]{idx}. {detail['document_title'][:60]}[/red]")
            console.print(f"   Project ID: {detail['project_id']}")
            console.print(f"   Reference Doc ID: {detail['reference_doc_id']}")
            console.print(f"   Error: {detail['error']}")
            console.print(f"   Path: {GOLD_BASE_PATH / detail['project_id'] / (detail['reference_doc_id'] + '.json')}")

    console.print(f"\n[bold green]✓ Document indexing complete![/bold green]")
    console.print(f"[dim]Indexed {total_chunks_indexed} chunks into OpenSearch[/dim]")


@app.command()
def verify(
    limit: int = typer.Option(10, "--limit", "-n", help="Number of records to show")
):
    """Verify indexing status and show sample indexed documents"""

    postgres_settings = settings_factory.create_postgres_settings()
    connection_string = postgres_settings.connection_string
    engine = create_engine(connection_string, echo=False)

    with Session(engine) as session:
        # Get status statistics
        statement = select(DocumentIndexingStatus)
        all_status = session.exec(statement).all()

        ready_count = sum(1 for s in all_status if s.status == ProcessingStatus.READY)
        processed_count = sum(1 for s in all_status if s.status == ProcessingStatus.PROCESSED)
        failed_count = sum(1 for s in all_status if s.status == ProcessingStatus.FAILED)

        console.print("[bold cyan]Indexing Status Summary:[/bold cyan]")
        console.print(f"  Total tracked: {len(all_status)}")
        console.print(f"  Ready: [yellow]{ready_count}[/yellow]")
        console.print(f"  Processed: [green]{processed_count}[/green]")
        console.print(f"  Failed: [red]{failed_count}[/red]")

        # Calculate total chunks indexed
        total_chunks = sum(s.indexed_chunks_count or 0 for s in all_status if s.indexed_chunks_count)
        console.print(f"  Total chunks indexed: [green]{total_chunks}[/green]")

        # Show sample processed documents
        if processed_count > 0:
            console.print(f"\n[bold cyan]Sample Indexed Documents (first {limit}):[/bold cyan]\n")

            processed_docs = [s for s in all_status if s.status == ProcessingStatus.PROCESSED][:limit]

            for i, status in enumerate(processed_docs, 1):
                console.print(f"[cyan]{i}. Project: {status.project_id[:20]}...[/cyan]")
                console.print(f"   Reference Doc: {status.reference_doc_id}")
                console.print(f"   Status: [green]{status.status.value}[/green]")
                console.print(f"   Indexed Chunks: {status.indexed_chunks_count}")
                console.print(f"   Vector Dimension: {status.vector_dimension}")
                console.print(f"   Index Name: {status.index_name}")
                console.print(f"   Execution Time: {status.execution_time:.2f}s")
                console.print()

        # Show failed documents
        if failed_count > 0:
            console.print(f"\n[bold yellow]Failed Documents (first 5):[/bold yellow]\n")

            failed_docs = [s for s in all_status if s.status == ProcessingStatus.FAILED][:5]

            for i, status in enumerate(failed_docs, 1):
                console.print(f"[yellow]{i}. Project: {status.project_id[:20]}...[/yellow]")
                console.print(f"   Error: {status.error_message[:100]}...")
                console.print()


@app.command()
def search(
    query: str = typer.Argument(..., help="Search query text"),
    limit: int = typer.Option(5, "--limit", "-n", help="Number of results to return"),
    project_id: Optional[str] = typer.Option(None, "--project-id", help="Filter by project ID")
):
    """Test semantic search on indexed documents"""
    
    console.print(f"[cyan]Searching for: '{query}'[/cyan]")
    if project_id:
        console.print(f"[cyan]Filtered by project: {project_id}[/cyan]")
    console.print()

    try:
        # Initialize Vector CRUD service
        crud_service = OpenSearchVectorCRUDServiceFactory.create_default()
        
        # Perform semantic search
        search_result = crud_service.semantic_search(
            query_text=query,
            limit=limit,
            project_id=project_id
        )
        
        if search_result.is_err():
            console.print(f"[red]Search failed: {search_result.err()}[/red]")
            raise typer.Exit(1)
        
        results = search_result.unwrap()
        
        if not results:
            console.print("[yellow]No results found[/yellow]")
            return
        
        console.print(f"[green]Found {len(results)} results:[/green]\n")
        
        for i, result in enumerate(results, 1):
            document = result["document"]
            score = result["score"]
            
            console.print(f"[bold cyan]{i}. Score: {score:.4f}[/bold cyan]")
            console.print(f"   Document: {document.document_title}")
            console.print(f"   Project: {document.project_id}")
            console.print(f"   Chunk: {document.chunk_index}/{document.chunk_id}")
            console.print(f"   Content: {document.content[:200]}...")
            console.print(f"   Hierarchy: {' > '.join(document.section_hierarchy[-2:]) if document.section_hierarchy else 'N/A'}")
            console.print()
            
    except Exception as e:
        console.print(f"[red]Error performing search: {e}[/red]")
        raise typer.Exit(1)


if __name__ == "__main__":
    app()