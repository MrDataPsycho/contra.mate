#!/usr/bin/env python3
"""
Tool to index document chunks from gold/ to OpenSearch vector store.

Pipeline:
1. Read chunked documents from gold layer (JSON files)
2. Convert chunks to VectorModel instances
3. Generate embeddings and index into OpenSearch using OpenSearchVectorCRUDService
4. Track processing status in document_indexing_status table

Usage:
    uv run python src/tools/index_chunks.py index
    uv run python src/tools/index_chunks.py index --limit 10
    uv run python src/tools/index_chunks.py index --batch-size 50
    uv run python src/tools/index_chunks.py verify
"""

import json
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, List, Dict, Any

import typer
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from sqlmodel import Session, create_engine, select
from loguru import logger

from contramate.dbs.models.contract import ContractAsmd
from contramate.dbs.models.document_status import (
    DocumentIndexingStatus,
    ProcessingStatus,
)
from contramate.models.vector import VectorModel, ContentSource
from contramate.services.opensearch_vector_crud_service import OpenSearchVectorCRUDServiceFactory
from contramate.utils.settings.factory import settings_factory

app = typer.Typer(help="Index document chunks to OpenSearch with status tracking")
console = Console()

# Paths
GOLD_BASE_PATH = Path("data/gold")


def find_gold_json_file(project_id: str, reference_doc_id: str) -> Optional[Path]:
    """
    Find chunked JSON file in gold directory.

    Args:
        project_id: Project identifier
        reference_doc_id: Document identifier

    Returns:
        Path to JSON file or None if not found
    """
    json_path = GOLD_BASE_PATH / project_id / f"{reference_doc_id}.json"

    if json_path.exists():
        return json_path

    return None


def load_chunks_from_json(json_path: Path) -> Dict[str, Any]:
    """
    Load chunked document from JSON file.

    Args:
        json_path: Path to JSON file

    Returns:
        Dictionary with document metadata and chunks
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def convert_chunks_to_vector_models(
    chunked_data: Dict[str, Any],
    document_title: Optional[str] = None
) -> List[VectorModel]:
    """
    Convert chunks from JSON to VectorModel instances (without embeddings).

    Args:
        chunked_data: Loaded JSON data from gold layer
        document_title: Optional document title from database

    Returns:
        List of VectorModel instances ready for embedding and indexing
    """
    vector_models = []

    project_id = chunked_data["project_id"]
    reference_doc_id = chunked_data["reference_doc_id"]
    contract_type = chunked_data.get("contract_type", "Unknown")

    # Use document title from database if provided, otherwise from JSON
    title = document_title or f"Contract_{reference_doc_id[:8]}"

    for idx, chunk in enumerate(chunked_data["chunks"]):
        # Create VectorModel without embeddings (will be auto-generated during insertion)
        # Use chunk_index + 1 as chunk_id to ensure unique IDs starting from 1
        vector_model = VectorModel(
            chunk_id=chunk.get("chunk_index", idx) + 1,  # Ensure chunk_id starts from 1
            project_id=project_id,
            reference_doc_id=reference_doc_id,
            document_title=title,
            display_name=None,
            content_source=ContentSource.upload,
            contract_type=contract_type,
            content=chunk["content"],
            chunk_index=chunk.get("chunk_index", idx),
            section_hierarchy=chunk.get("section_hierarchy", []),
            char_start=chunk["char_start"],
            char_end=chunk["char_end"],
            token_count=chunk["token_count"],
            has_tables=chunk.get("has_tables", False),
            vector=[],  # Empty vector - will be auto-generated by CRUD service
            created_at=datetime.now(timezone.utc)
        )
        vector_models.append(vector_model)

    return vector_models


@app.command()
def index(
    limit: Optional[int] = typer.Option(
        None,
        "--limit",
        "-l",
        help="Limit number of documents to process"
    ),
    skip_existing: bool = typer.Option(
        True,
        "--skip-existing/--reprocess",
        help="Skip documents that are already indexed"
    ),
    batch_size: int = typer.Option(
        50,
        "--batch-size",
        "-b",
        help="Number of chunks to index in a single batch operation"
    ),
    delay_seconds: float = typer.Option(
        0.0,
        "--delay",
        "-d",
        help="Delay in seconds between processing each document (useful for API rate limits)"
    )
):
    """Index document chunks from gold layer to OpenSearch with status tracking"""

    # Create database connection
    postgres_settings = settings_factory.create_postgres_settings()
    connection_string = postgres_settings.connection_string
    engine = create_engine(connection_string, echo=False)

    # Initialize OpenSearch CRUD service
    console.print("[cyan]Initializing OpenSearch CRUD service...[/cyan]")
    try:
        crud_service = OpenSearchVectorCRUDServiceFactory.create_default()
        console.print("[green]✓ OpenSearch CRUD service initialized[/green]\n")
    except Exception as e:
        console.print(f"[red]✗ Failed to initialize OpenSearch service: {e}[/red]")
        raise typer.Exit(1)

    # Statistics
    total_documents = 0
    processed = 0
    failed = 0
    skipped = 0
    not_found = 0
    total_chunks_indexed = 0
    failed_details = []  # Track failed documents with details

    console.print(f"[cyan]Starting document indexing...[/cyan]")
    console.print(f"[cyan]Source: {GOLD_BASE_PATH}[/cyan]")
    console.print(f"[cyan]Batch size: {batch_size}[/cyan]")
    if delay_seconds > 0:
        console.print(f"[cyan]Delay between documents: {delay_seconds}s[/cyan]")
    console.print()

    with Session(engine) as session:
        # Get all documents from contract_asmd
        statement = select(ContractAsmd)
        if limit:
            statement = statement.limit(limit)

        contracts = session.exec(statement).all()
        total_documents = len(contracts)

        console.print(f"[cyan]Found {total_documents} documents to process[/cyan]\n")

        # Counter for progress reporting
        doc_counter = 0

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console
        ) as progress:
            task = progress.add_task("Indexing documents...", total=total_documents)

            for contract in contracts:
                doc_counter += 1
                project_id = contract.project_id
                reference_doc_id = contract.reference_doc_id

                # Check if already indexed
                if skip_existing:
                    existing_status = session.exec(
                        select(DocumentIndexingStatus).where(
                            DocumentIndexingStatus.project_id == project_id,
                            DocumentIndexingStatus.reference_doc_id == reference_doc_id,
                            DocumentIndexingStatus.status == ProcessingStatus.PROCESSED
                        )
                    ).first()

                    if existing_status:
                        skipped += 1
                        progress.update(task, advance=1)
                        continue

                # Find JSON file in gold layer
                json_file = find_gold_json_file(project_id, reference_doc_id)
                if not json_file:
                    not_found += 1
                    progress.update(task, advance=1)
                    continue

                # Initialize status record
                status_record = DocumentIndexingStatus(
                    project_id=project_id,
                    reference_doc_id=reference_doc_id,
                    status=ProcessingStatus.READY,
                    created_at=datetime.now(timezone.utc),
                    updated_at=datetime.now(timezone.utc)
                )

                # Check if status record already exists
                existing = session.exec(
                    select(DocumentIndexingStatus).where(
                        DocumentIndexingStatus.project_id == project_id,
                        DocumentIndexingStatus.reference_doc_id == reference_doc_id
                    )
                ).first()

                if existing:
                    status_record = existing
                    status_record.status = ProcessingStatus.READY
                    status_record.updated_at = datetime.now(timezone.utc)
                else:
                    session.add(status_record)

                session.commit()

                # Index document chunks
                start_time = time.time()
                try:
                    # Step 1: Load chunks from JSON
                    chunked_data = load_chunks_from_json(json_file)

                    # Step 2: Convert to VectorModel instances
                    vector_models = convert_chunks_to_vector_models(
                        chunked_data,
                        document_title=contract.document_title
                    )

                    if not vector_models:
                        raise RuntimeError("No chunks found in document")

                    # Step 3: Bulk index with embeddings using CRUD service
                    # The service will auto-generate embeddings if vectors are empty
                    bulk_result = crud_service.bulk_upsert_documents(
                        documents=vector_models,
                        auto_embed=True  # Auto-generate embeddings
                    )

                    if bulk_result.is_err():
                        raise RuntimeError(f"Bulk indexing failed: {bulk_result.err()}")

                    result_stats = bulk_result.unwrap()
                    chunks_indexed = result_stats["success"]
                    chunks_failed = result_stats["failed"]

                    if chunks_failed > 0:
                        console.print(
                            f"[yellow]⚠ {chunks_failed}/{len(vector_models)} chunks failed for: "
                            f"{contract.document_title[:40] if contract.document_title else 'Unknown'}...[/yellow]"
                        )

                    execution_time = time.time() - start_time

                    # Get index name from app settings
                    app_config = settings_factory.create_app_settings()
                    index_name = app_config.default_index_name

                    # Get vector dimension from first vector model (after embedding)
                    vector_dim = None
                    if vector_models and vector_models[0].vector:
                        vector_dim = len(vector_models[0].vector)

                    # Update status to PROCESSED
                    status_record.status = ProcessingStatus.PROCESSED
                    status_record.indexed_chunks_count = chunks_indexed
                    status_record.vector_dimension = vector_dim
                    status_record.index_name = index_name
                    status_record.execution_time = execution_time
                    status_record.updated_at = datetime.now(timezone.utc)
                    session.add(status_record)
                    session.commit()

                    processed += 1
                    total_chunks_indexed += chunks_indexed

                    # Add delay after successful processing
                    if delay_seconds > 0:
                        time.sleep(delay_seconds)

                except Exception as e:
                    execution_time = time.time() - start_time

                    # Update status to FAILED
                    status_record.status = ProcessingStatus.FAILED
                    status_record.execution_time = execution_time
                    status_record.error_message = str(e)[:1000]
                    status_record.updated_at = datetime.now(timezone.utc)
                    session.add(status_record)
                    session.commit()

                    failed += 1
                    # Track failed document details
                    failed_details.append({
                        "project_id": project_id,
                        "reference_doc_id": reference_doc_id,
                        "document_title": contract.document_title or "Unknown",
                        "error": str(e)[:200]
                    })
                    console.print(
                        f"[red]✗ Failed: {contract.document_title[:60] if contract.document_title else 'Unknown'}...[/red]"
                    )

                progress.update(task, advance=1)

                # Print progress every 10 documents
                if doc_counter % 10 == 0:
                    progress.stop()
                    console.print(f"\n[bold cyan]Progress Update:[/bold cyan]")
                    console.print(f"  Processed: {doc_counter}/{total_documents} documents")
                    console.print(
                        f"  Success: [green]{processed}[/green] | "
                        f"Failed: [red]{failed}[/red] | "
                        f"Skipped: [yellow]{skipped}[/yellow] | "
                        f"Not Found: [yellow]{not_found}[/yellow]"
                    )
                    console.print(f"  Total chunks indexed: [green]{total_chunks_indexed}[/green]\n")
                    progress.start()

    # Print summary
    console.print("\n[bold cyan]Indexing Summary:[/bold cyan]")
    console.print(f"  Total documents: {total_documents}")
    console.print(f"  Successfully indexed: [green]{processed}[/green]")
    console.print(f"  Total chunks indexed: [green]{total_chunks_indexed}[/green]")
    console.print(f"  Failed: [red]{failed}[/red]")
    console.print(f"  Skipped (already indexed): [yellow]{skipped}[/yellow]")
    console.print(f"  JSON files not found: [yellow]{not_found}[/yellow]")

    # Show failed document details
    if failed_details:
        console.print("\n[bold red]Failed Documents:[/bold red]")
        for idx, detail in enumerate(failed_details, 1):
            console.print(f"\n[red]{idx}. {detail['document_title'][:60]}[/red]")
            console.print(f"   Project ID: {detail['project_id']}")
            console.print(f"   Reference Doc ID: {detail['reference_doc_id']}")
            console.print(f"   Error: {detail['error']}")
            console.print(f"   Path: {GOLD_BASE_PATH / detail['project_id'] / detail['reference_doc_id']}.json")

    console.print(f"\n[bold green]✓ Indexing complete![/bold green]")


@app.command()
def verify(
    limit: int = typer.Option(10, "--limit", "-n", help="Number of records to show")
):
    """Verify indexing status and show indexed document statistics"""

    postgres_settings = settings_factory.create_postgres_settings()
    connection_string = postgres_settings.connection_string
    engine = create_engine(connection_string, echo=False)

    # Initialize OpenSearch service to check actual index
    try:
        crud_service = OpenSearchVectorCRUDServiceFactory.create_default()
        count_result = crud_service.get_document_count()
        opensearch_count = count_result.unwrap() if count_result.is_ok() else "Error"
    except Exception as e:
        opensearch_count = f"Error: {str(e)[:50]}"

    with Session(engine) as session:
        # Get status statistics
        statement = select(DocumentIndexingStatus)
        all_status = session.exec(statement).all()

        ready_count = sum(1 for s in all_status if s.status == ProcessingStatus.READY)
        processed_count = sum(1 for s in all_status if s.status == ProcessingStatus.PROCESSED)
        failed_count = sum(1 for s in all_status if s.status == ProcessingStatus.FAILED)

        # Calculate total indexed chunks
        total_indexed_chunks = sum(
            s.indexed_chunks_count for s in all_status
            if s.status == ProcessingStatus.PROCESSED and s.indexed_chunks_count
        )

        console.print("[bold cyan]Indexing Status Summary:[/bold cyan]")
        console.print(f"  Total tracked documents: {len(all_status)}")
        console.print(f"  Ready: [yellow]{ready_count}[/yellow]")
        console.print(f"  Processed: [green]{processed_count}[/green]")
        console.print(f"  Failed: [red]{failed_count}[/red]")
        console.print(f"  Total chunks indexed: [green]{total_indexed_chunks}[/green]")
        console.print(f"  OpenSearch index count: [cyan]{opensearch_count}[/cyan]")

        # Show sample processed documents
        if processed_count > 0:
            console.print(f"\n[bold cyan]Sample Indexed Documents (first {limit}):[/bold cyan]\n")

            processed_docs = [s for s in all_status if s.status == ProcessingStatus.PROCESSED][:limit]

            for i, status in enumerate(processed_docs, 1):
                console.print(f"[cyan]{i}. Project: {status.project_id[:20]}...[/cyan]")
                console.print(f"   Reference Doc: {status.reference_doc_id}")
                console.print(f"   Status: [green]{status.status.value}[/green]")
                console.print(f"   Indexed Chunks: {status.indexed_chunks_count}")
                console.print(f"   Vector Dimension: {status.vector_dimension}")
                console.print(f"   Index Name: {status.index_name}")
                console.print(f"   Execution Time: {status.execution_time:.2f}s")
                console.print()

        # Show failed documents
        if failed_count > 0:
            console.print(f"\n[bold yellow]Failed Documents (first 5):[/bold yellow]\n")

            failed_docs = [s for s in all_status if s.status == ProcessingStatus.FAILED][:5]

            for i, status in enumerate(failed_docs, 1):
                console.print(f"[yellow]{i}. Project: {status.project_id[:20]}...[/yellow]")
                console.print(f"   Reference Doc: {status.reference_doc_id}")
                console.print(f"   Error: {status.error_message[:100]}...")
                console.print()


@app.command()
def reset_status(
    project_id: Optional[str] = typer.Option(None, "--project-id", help="Reset specific project"),
    reference_doc_id: Optional[str] = typer.Option(None, "--reference-doc-id", help="Reset specific document"),
    all_docs: bool = typer.Option(False, "--all", help="Reset all documents (use with caution)")
):
    """Reset indexing status for reprocessing (does not delete from OpenSearch)"""

    postgres_settings = settings_factory.create_postgres_settings()
    connection_string = postgres_settings.connection_string
    engine = create_engine(connection_string, echo=False)

    if not all_docs and not (project_id and reference_doc_id):
        console.print("[red]Error: Must specify either --all or both --project-id and --reference-doc-id[/red]")
        raise typer.Exit(1)

    with Session(engine) as session:
        if all_docs:
            # Confirm with user
            confirm = typer.confirm("Are you sure you want to reset ALL indexing statuses?")
            if not confirm:
                console.print("[yellow]Operation cancelled[/yellow]")
                return

            # Reset all statuses
            statement = select(DocumentIndexingStatus)
            all_status = session.exec(statement).all()

            for status in all_status:
                status.status = ProcessingStatus.READY
                status.updated_at = datetime.now(timezone.utc)
                session.add(status)

            session.commit()
            console.print(f"[green]✓ Reset {len(all_status)} document statuses to READY[/green]")

        else:
            # Reset specific document
            statement = select(DocumentIndexingStatus).where(
                DocumentIndexingStatus.project_id == project_id,
                DocumentIndexingStatus.reference_doc_id == reference_doc_id
            )
            status = session.exec(statement).first()

            if not status:
                console.print("[yellow]No status record found for specified document[/yellow]")
                return

            status.status = ProcessingStatus.READY
            status.updated_at = datetime.now(timezone.utc)
            session.add(status)
            session.commit()

            console.print(f"[green]✓ Reset status for document {reference_doc_id} to READY[/green]")


if __name__ == "__main__":
    app()
